{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:28:22.679513Z",
     "start_time": "2025-03-21T12:28:21.737321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor, defer\n",
    "import json"
   ],
   "id": "f28674e80cda5908",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T12:31:09.714075Z",
     "start_time": "2025-03-21T12:30:38.466057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "#from trl import SFTTrainer\n",
    "#from peft import LoraConfig\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import pinecone as pinecone"
   ],
   "id": "9df6ce6ff800a2b2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adity\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:30:10.676362Z",
     "start_time": "2025-03-21T13:30:09.075391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ],
   "id": "ca47f6b0a50b932e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:17:56.869619Z",
     "start_time": "2025-03-14T19:17:56.859311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "import copy\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ],
   "id": "da060161eac23e03",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T17:46:41.674570Z",
     "start_time": "2025-03-21T17:46:40.416606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the environment variable for the token\n",
    "token = \"hf_PkglIyXxQdtvOlAp789chqjAoCzmVgWNwnDRH\"  # Replace with your actual token\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define paths\n",
    "model_save_path = r\"C://Users//adity//Downloads//gemma_model\"\n",
    "tokenizer_save_path = r\"C://Users//adity//Downloads//gemma_tokenizer\"\n",
    "\n",
    "# Load the tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_save_path)\n",
    "\n",
    "print(model)\n",
    "print(tokenizer)\n",
    "os.environ['WANDB_DISABLED'] = 'false'\n",
    "\n",
    "# **Load the data, ensuring the path is correct and the file exists**\n",
    "data = pd.read_csv(\"C://Users//adity//Downloads//mbti_1.csv\")"
   ],
   "id": "67b701a3f2867a2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49153, 576, padding_idx=49152)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49153, bias=False)\n",
      ")\n",
      "GPT2TokenizerFast(name_or_path='C://Users//adity//Downloads//gemma_tokenizer', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': 'ï¿½', 'pad_token': '<|PAD_TOKEN|>', 'additional_special_tokens': ['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<repo_name>', '<reponame>', '<file_sep>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t24211: AddedToken(\"ï¿½\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49152: AddedToken(\"<|PAD_TOKEN|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:45:05.655353Z",
     "start_time": "2025-03-14T16:45:05.641961Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package objects:\n",
      "\n",
      "NAME\n",
      "    objects - A corpus of object images.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Object\n",
      "\n",
      "    class Object(builtins.object)\n",
      "     |  Object(path)\n",
      "     |\n",
      "     |  Photgraph of an object.\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, path)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |\n",
      "     |  base64\n",
      "     |      Load an image.\n",
      "     |\n",
      "     |  data_uri\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "FUNCTIONS\n",
      "    download()\n",
      "        Download the corpus.\n",
      "\n",
      "    objects(idx)\n",
      "\n",
      "DATA\n",
      "    object_dir = r'C:\\Users\\adity\\PyCharmMiscProject\\.venv\\Lib\\site-packag...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\adity\\pycharmmiscproject\\.venv\\lib\\site-packages\\objects\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "import objects  # Replace with the actual module name\n",
    "help(objects)"
   ],
   "id": "ef1de89d06a3adfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:45:20.226239Z",
     "start_time": "2025-03-14T16:45:15.056175Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from pinecone import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import uuid\n",
    "from uuid import uuid4\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"pcsk_2f3233n1oB_598VChT9gwtgN6wia9h8GdhwC2wK3NbhZi3BGRLoovzpgCLjguw32y4qaxQHqyk\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"sk-proj-tuKG0k3CDi39ojvnzqZ_3ANhQax7bBqI5_jexdhnH-rrkcYo9900dq5sYhab3OPi2crSfD2LLzVAlRT3BlbkFJxvqREIBGDrY6M7QRaHXAnXe45C-AK6Q8u_OCMoZQhoP--mtaU3e_VglNWRtzjnwVwBkB7c9zoA\")\n",
    "INDEX_NAME=\"mark2\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY,environment='us-east-1')  # Example initialization, modify as per actual function\n",
    "index = pc.Index(INDEX_NAME)\n",
    "PROXYCURL_API_KEY = os.getenv(\"PROXYCURL_API_KEY\", \"VFM38hNSeR4CFXmVw4WvQw  \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ],
   "id": "7b3cd7c9b3f2fded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:31:40.834522Z",
     "start_time": "2025-03-14T17:31:19.065318Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "service = Service(GeckoDriverManager().install()) #DO NOT RUN IT\n",
    "driver = webdriver.Firefox(service=service)"
   ],
   "id": "422361340b5b8a2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:32:22.440822Z",
     "start_time": "2025-03-14T17:32:22.427669Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "#from anticaptchaofficial.recaptchaproxyless import *\n",
    "def byPass(url):\n",
    "    service = Service(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=service)\n",
    "    page=driver.get(url)\n",
    "    sitekey=driver.find_element(By.XPATH,'//*[@id=\"recaptcha-demo\"]').get_attribute('outerHTML')\n",
    "    sitekey_clean=sitekey.split('\"data-callback')[0].split['data-sitekey=\"'][1]\n",
    "    print(sitekey_clean)\n",
    "    solver=recaptchaV2Proxyless()\n",
    "    solver.set_verbose(1)\n",
    "    solver.set_key(os.environ[\"anticaptcha_api_key\"])\n",
    "    solver.set_website_url(url)\n",
    "    solver.set_website_key(sitekey_clean)\n",
    "    g_response=solver.solve_and_return_solution()\n",
    "    if g_response!=0:\n",
    "        print(g_response)\n",
    "    else:\n",
    "        print(\"task finished with error\"+solver.error_code)\n",
    "    driver.execute_script('var element=document.getElementById(\"g-recaptcha-response\"); element.style.display=\"\"')\n",
    "\n",
    "    driver.execute_script(\n",
    "    \"\"\"document.getElementById(\"g-recaptcha-response\").innerHTML = arguments[0]\"\"\",\n",
    "    g_response\n",
    ")\n",
    "\n",
    "    driver.execute_script('var element=document.getElementById(\"g-recaptcha-response\"); element.style.display=\"none\"')\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"g-recaptcha-response\"]').click()\n",
    "    time.sleep(20)\n",
    "\n"
   ],
   "id": "73e86cb4f4f52a3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:13:45.717829Z",
     "start_time": "2025-03-14T19:13:23.297016Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 221\u001B[0m\n\u001B[0;32m    218\u001B[0m     reactor\u001B[38;5;241m.\u001B[39mrun()\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 221\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[17], line 210\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    207\u001B[0m profile_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease input your LinkedIn profile URL: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    209\u001B[0m \u001B[38;5;66;03m# Use Selenium to bypass the captcha and obtain authenticated cookies\u001B[39;00m\n\u001B[1;32m--> 210\u001B[0m captcha_cookies \u001B[38;5;241m=\u001B[39m \u001B[43mbyPass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprofile_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m captcha_cookies \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaptcha bypass failed. Exiting.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[17], line 13\u001B[0m, in \u001B[0;36mbyPass\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#driver = webdriver.Edge(service=service)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m driver\u001B[38;5;241m.\u001B[39mget(url)\n\u001B[1;32m---> 13\u001B[0m \u001B[43mtime\u001B[49m\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m5\u001B[39m)  \u001B[38;5;66;03m# Allow the page to load\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Extract the recaptcha element\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'time' is not defined"
     ]
    }
   ],
   "execution_count": 17,
   "source": [
    "########################################\n",
    "# Part 1. Selenium Code for Captcha Bypass and Cookie Extraction\n",
    "########################################\n",
    "def byPass(url):\n",
    "    # Initialize Firefox with GeckoDriver\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference(\"general.useragent.override\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko\")\n",
    "    service = Service(GeckoDriverManager().install())\n",
    "    #service = Service(EdgeChromiumDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=service)\n",
    "    #driver = webdriver.Edge(service=service)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow the page to load\n",
    "    # Extract the recaptcha element\n",
    "    try:\n",
    "        recaptcha_element = driver.find_element(By.XPATH, '//*[@id=\"recaptcha-demo\"]')\n",
    "    except Exception as e:\n",
    "        print(\"Error finding the recaptcha element:\", e)\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    sitekey_html = recaptcha_element.get_attribute(\"outerHTML\")\n",
    "    try:\n",
    "        # Parse the data-sitekey from the element's HTML\n",
    "        sitekey_clean = sitekey_html.split('data-sitekey=\"')[1].split('\"')[0]\n",
    "    except IndexError:\n",
    "        print(\"Could not extract site key!\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    print(\"Site key extracted:\", sitekey_clean)\n",
    "\n",
    "    # Solve the captcha using anticaptcha\n",
    "    solver = recaptchaV2Proxyless()\n",
    "    solver.set_verbose(1)\n",
    "    solver.set_key(os.environ[\"ANTICAPTCHA_API_KEY\"])  # Set your anticaptcha API key in your environment\n",
    "    solver.set_website_url(url)\n",
    "    solver.set_website_key(sitekey_clean)\n",
    "    g_response = solver.solve_and_return_solution()\n",
    "\n",
    "    if g_response != 0:\n",
    "        print(\"Captcha solved, response:\", g_response)\n",
    "    else:\n",
    "        print(\"Error solving captcha: \" + solver.error_code)\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # Inject the captcha response into the page's recaptcha response element\n",
    "    driver.execute_script('document.getElementById(\"g-recaptcha-response\").style.display=\"\"')\n",
    "    driver.execute_script('document.getElementById(\"g-recaptcha-response\").innerHTML = arguments[0]', g_response)\n",
    "    driver.execute_script('document.getElementById(\"g-recaptcha-response\").style.display=\"none\"')\n",
    "\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '//*[@id=\"g-recaptcha-response\"]').click()\n",
    "    except Exception as e:\n",
    "        print(\"Error clicking captcha element:\", e)\n",
    "\n",
    "    time.sleep(20)  # Wait for any post-captcha processing\n",
    "\n",
    "    # Extract cookies from Selenium's session and close the browser\n",
    "    cookies = driver.get_cookies()\n",
    "    driver.quit()\n",
    "    # Convert the cookies list to a dictionary format for Scrapy\n",
    "    cookies_dict = {cookie['name']: cookie['value'] for cookie in cookies}\n",
    "    return cookies_dict\n",
    "\n",
    "########################################\n",
    "# Part 2. Scrapy Spider for LinkedIn Profile Scraping\n",
    "########################################\n",
    "# Global list to collect data\n",
    "data_collected = []\n",
    "\n",
    "def scrape_linkedin_profiles(profile_list, cookies):\n",
    "    class LinkedInPeopleProfileSpider(scrapy.Spider):\n",
    "        name = \"linkedin_people_profile\"\n",
    "\n",
    "        def __init__(self, cookies, *args, **kwargs):\n",
    "            super(LinkedInPeopleProfileSpider, self).__init__(*args, **kwargs)\n",
    "            self.cookies = cookies\n",
    "\n",
    "        def start_requests(self):\n",
    "            for profile in profile_list:\n",
    "                # Construct the full LinkedIn profile URL (adjust if your input is already a URL)\n",
    "                linkedin_people_url = f'https://www.linkedin.com/in/{profile}/'\n",
    "                yield scrapy.Request(\n",
    "                    url=linkedin_people_url,\n",
    "                    callback=self.parse_profile,\n",
    "                    cookies=self.cookies,  # Use the cookies from our Selenium session\n",
    "                    meta={'profile': profile, 'linkedin_url': linkedin_people_url}\n",
    "                )\n",
    "\n",
    "        def parse_profile(self, response):\n",
    "            item = {\n",
    "                'profile': response.meta['profile'],\n",
    "                'url': response.meta['linkedin_url'],\n",
    "                'name': response.css(\"section.top-card-layout h1::text\").get(default='').strip(),\n",
    "                'description': response.css(\"section.top-card-layout h2::text\").get(default='').strip(),\n",
    "                'location': response.css('div.top-card__subline-item::text').get(default='').strip(),\n",
    "                'followers': '',\n",
    "                'connections': '',\n",
    "                'about': response.css('section.summary div.core-section-container__content p::text').get(default=''),\n",
    "                'experience': [],\n",
    "                'education': []\n",
    "            }\n",
    "\n",
    "            # Extract followers and connections\n",
    "            for span_text in response.css('span.top-card__subline-item::text').getall():\n",
    "                if 'followers' in span_text:\n",
    "                    item['followers'] = span_text.replace(' followers', '').strip()\n",
    "                if 'connections' in span_text:\n",
    "                    item['connections'] = span_text.replace(' connections', '').strip()\n",
    "\n",
    "            # Extract experience\n",
    "            for block in response.css('li.experience-item'):\n",
    "                experience = {\n",
    "                    'organisation_profile': block.css('h4 a::attr(href)').get(default='').split('?')[0],\n",
    "                    'location': block.css('p.experience-item__location::text').get(default='').strip(),\n",
    "                    'description': block.css('p.show-more-less-text__text--more::text').get(default='').strip()\n",
    "                }\n",
    "                date_ranges = block.css('span.date-range time::text').getall()\n",
    "                experience['start_time'] = date_ranges[0] if date_ranges else ''\n",
    "                experience['end_time'] = date_ranges[1] if len(date_ranges) > 1 else 'present'\n",
    "                experience['duration'] = block.css('span.date-range__duration::text').get(default='')\n",
    "                item['experience'].append(experience)\n",
    "\n",
    "            # Extract education\n",
    "            for block in response.css('li.education__list-item'):\n",
    "                education = {\n",
    "                    'organisation': block.css('h3::text').get(default='').strip(),\n",
    "                    'organisation_profile': block.css('a::attr(href)').get(default='').split('?')[0],\n",
    "                    'course_details': ' '.join(block.css('h4 span::text').getall()).strip(),\n",
    "                    'description': block.css('div.education__item--details p::text').get(default='').strip()\n",
    "                }\n",
    "                date_ranges = block.css('span.date-range time::text').getall()\n",
    "                education['start_time'] = date_ranges[0] if date_ranges else ''\n",
    "                education['end_time'] = date_ranges[1] if len(date_ranges) > 1 else 'present'\n",
    "                item['education'].append(education)\n",
    "\n",
    "            data_collected.append(item)\n",
    "\n",
    "    runner = CrawlerRunner()\n",
    "    # Start the crawl with the provided cookies\n",
    "    return runner.crawl(LinkedInPeopleProfileSpider, cookies=cookies)\n",
    "\n",
    "@defer.inlineCallbacks\n",
    "def extraction_from_linkedin(profile_url, cookies):\n",
    "    \"\"\"\n",
    "    Kicks off the Scrapy crawl using the provided LinkedIn profile URL and cookies.\n",
    "    The profile_list is created by extracting the profile identifier from the URL.\n",
    "    \"\"\"\n",
    "    # Extract the profile ID from the URL (removing any trailing slashes)\n",
    "    profile_id = profile_url.rstrip('/').split(\"/\")[-1]\n",
    "    profile_list = [profile_id]\n",
    "    yield scrape_linkedin_profiles(profile_list, cookies)\n",
    "    defer.returnValue(json.dumps(data_collected, indent=4))\n",
    "\n",
    "########################################\n",
    "# Part 3. Document Preparation and Vector Store Creation\n",
    "########################################\n",
    "def load_and_prepare_documents(text):\n",
    "    \"\"\"Load and split text into chunks for better embedding representation.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    return texts\n",
    "\n",
    "def create_vector_store(docs, user_id, username):\n",
    "    \"\"\"\n",
    "    Creates OpenAI embeddings for the documents and upserts them into a vector store.\n",
    "\n",
    "    NOTE: Make sure you have configured your vector store (e.g., Pinecone) and have an index.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    doc_embeddings = embeddings.embed_documents(docs)\n",
    "    metadatas = [{\"user_id\": str(user_id), \"username\": username} for _ in docs]\n",
    "    vectors = []\n",
    "    for i, (embedding, metadata) in enumerate(zip(doc_embeddings, metadatas)):\n",
    "        vector = {\n",
    "            'id': f'{user_id}_{i}',\n",
    "            'values': embedding,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        vectors.append(vector)\n",
    "    # Ensure that you have defined and configured your vector store index (for example, a Pinecone index)\n",
    "    # Example: index.upsert(vectors)\n",
    "    print(\"Upserting the following vectors into your vector store:\")\n",
    "    print(vectors)\n",
    "\n",
    "def process_data(extracted_text, user_id, username):\n",
    "    \"\"\"Process extracted text and store it in the vector database.\"\"\"\n",
    "    if not extracted_text:\n",
    "        print(\"Failed to extract data from LinkedIn.\")\n",
    "        reactor.stop()\n",
    "        return\n",
    "\n",
    "    docs = load_and_prepare_documents(extracted_text)\n",
    "    create_vector_store(docs, user_id, username)\n",
    "    print(\"LinkedIn data successfully stored in the vector database!\")\n",
    "    reactor.stop()  # Stop the reactor once processing is complete\n",
    "\n",
    "########################################\n",
    "# Part 4. Main Execution Flow\n",
    "########################################\n",
    "def main():\n",
    "    user_id = uuid.uuid4()\n",
    "    username = input(\"Enter your username: \")\n",
    "    profile_url = input(\"Please input your LinkedIn profile URL: \")\n",
    "\n",
    "    # Use Selenium to bypass the captcha and obtain authenticated cookies\n",
    "    captcha_cookies = byPass(profile_url)\n",
    "    if captcha_cookies is None:\n",
    "        print(\"Captcha bypass failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Start extraction using Scrapy with the Selenium cookies\n",
    "    d = extraction_from_linkedin(profile_url, captcha_cookies)\n",
    "    d.addCallback(lambda extracted_text: process_data(extracted_text, user_id, username))\n",
    "    reactor.run()\n",
    "    question=Question()\n",
    "    question.intializer(d)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9ad81bce452f729e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T17:51:33.432291Z",
     "start_time": "2025-03-21T17:51:33.420149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self._start_time is None:\n",
    "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")\n",
    "class Question():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def ask_question(self,question,json_data):\n",
    "        result = llm_chain(question,json_data)\n",
    "        print(result['question'])\n",
    "        print(\"\")\n",
    "        print(result['text'])\n",
    "    def initializer(self, json_data):\n",
    "        model_save_path = r\"C://Users//adity//Downloads//gemma_model\"\n",
    "        tokenizer_save_path = r\"C://Users//adity//Downloads//gemma_tokenizer\"\n",
    "# Load the tokenizer and model from the saved directory\n",
    "        model_id=model,\n",
    "        task=\"text2text-generation\",\n",
    "        model_kwargs={\"temperature\": 0, \"max_length\": 1000},)\n",
    "        json_str = json.dumps(json_data, indent=2)\n",
    "        template = f\"\"\"\n",
    "        \"Below is a JSON file content:\\n\\n\"\n",
    "        \"```json\\n\" + {json_str} + \"\\n```\\n\\n\"\n",
    "        \"Please analyze the structure, key components, and any notable content details.\"\n",
    "        You are a friendly chatbot assistant that responds conversationally to users' questions.\n",
    "        Keep the answers short, unless specifically asked by the user to elaborate on something.\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "        while Timer():\n",
    "            self.ask_question(input(\"Enter your question\"),json_str)"
   ],
   "id": "7ce23f1722e32b75",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T06:02:01.805008Z",
     "start_time": "2025-03-08T06:02:01.792241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ],
   "id": "2d5a35dca569932d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.act_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.act_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.act_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.act_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.mlp.act_fn\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.post_attention_layernorm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.mlp.act_fn\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.post_attention_layernorm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.mlp\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.mlp.act_fn\n",
      "model.layers.24.input_layernorm\n",
      "model.layers.24.post_attention_layernorm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.mlp\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.mlp.act_fn\n",
      "model.layers.25.input_layernorm\n",
      "model.layers.25.post_attention_layernorm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.mlp\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.mlp.act_fn\n",
      "model.layers.26.input_layernorm\n",
      "model.layers.26.post_attention_layernorm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.mlp\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.mlp.act_fn\n",
      "model.layers.27.input_layernorm\n",
      "model.layers.27.post_attention_layernorm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.mlp\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.mlp.act_fn\n",
      "model.layers.28.input_layernorm\n",
      "model.layers.28.post_attention_layernorm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.mlp\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.mlp.act_fn\n",
      "model.layers.29.input_layernorm\n",
      "model.layers.29.post_attention_layernorm\n",
      "model.norm\n",
      "model.rotary_emb\n",
      "lm_head\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T06:02:05.978893Z",
     "start_time": "2025-03-08T06:02:05.955906Z"
    }
   },
   "cell_type": "code",
   "source": "model.resize_token_embeddings(len(tokenizer))",
   "id": "ca71e2b1d964d215",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(49153, 576, padding_idx=49152)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T06:02:08.669106Z",
     "start_time": "2025-03-08T06:02:08.649167Z"
    }
   },
   "cell_type": "code",
   "source": "data.head",
   "id": "9064d90f82571ad4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       type                                              posts\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...\n",
       "...    ...                                                ...\n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n",
       "8671  ENFP  'So...if this thread already exists someplace ...\n",
       "8672  INTP  'So many questions when i do these things.  I ...\n",
       "8673  INFP  'I am very conflicted right now when it comes ...\n",
       "8674  INFP  'It has been too long since I have been on per...\n",
       "\n",
       "[8675 rows x 2 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T06:29:43.227849Z",
     "start_time": "2025-03-08T06:29:33.161524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Assuming you've loaded your CSV file into a DataFrame like this:\n",
    "# data = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Templates for formatting\n",
    "prompt_template = \"\"\"Hello! Below is an instruction that describes a task. Write a response that appropriately completes the request in normal English. Instruction: {instruction}\\n Response:\"\"\"\n",
    "answer_template = \"\"\"{response}\"\"\"\n",
    "\n",
    "# Function to process each row\n",
    "def _add_text(row):\n",
    "    instruction = row.get(\"type\")\n",
    "    response = row.get(\"posts\")\n",
    "\n",
    "    # Check if both exist, else raise error\n",
    "    if pd.isnull(instruction):\n",
    "        raise ValueError(f\"Expected an instruction in: {row}\")\n",
    "    if pd.isnull(response):\n",
    "        raise ValueError(f\"Expected a response in: {row}\")\n",
    "\n",
    "    row[\"prompt\"] = prompt_template.format(instruction=instruction)\n",
    "    row[\"answer\"] = answer_template.format(response=response)\n",
    "    row[\"text\"] = row[\"prompt\"] + row[\"answer\"]\n",
    "    return row\n",
    "# Apply the function to each row of the DataFrame\n",
    "data = data.apply(_add_text, axis=1)\n",
    "# Display the first row\n",
    "print(data.iloc[0])"
   ],
   "id": "18b96ab8bd72e788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                                                   INFJ\n",
      "posts     'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "prompt    Hello! Below is an instruction that describes ...\n",
      "answer    'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "text      Hello! Below is an instruction that describes ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T06:30:08.445837Z",
     "start_time": "2025-03-08T06:29:53.081998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Entire function for tokenization\n",
    "MAX_LENGTH=256\n",
    "dataset = Dataset.from_pandas(data)\n",
    "def _preprocess_batch(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch['text'],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    model_inputs['labels'] = copy.deepcopy(model_inputs['input_ids'])\n",
    "    return model_inputs\n",
    "\n",
    "_preprocessing_function = partial(_preprocess_batch)\n",
    "encoded_dataset = dataset.map(\n",
    "    _preprocessing_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "processed_dataset = encoded_dataset.filter(lambda rec: len(rec['input_ids']) <= MAX_LENGTH)\n",
    "split_dataset = processed_dataset.train_test_split(test_size=0.1, seed=0)  # Adjust test_size as needed\n",
    "print(split_dataset)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    pad_to_multiple_of=8,\n",
    "    padding='max_length',\n",
    ")"
   ],
   "id": "3cb8979203ff0d49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8675/8675 [00:09<00:00, 873.98 examples/s]\n",
      "Filter: 100%|██████████| 8675/8675 [00:03<00:00, 2490.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7807\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 868\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T06:32:54.166447Z",
     "start_time": "2025-03-08T06:32:53.817085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "#REMEMBER THIS FUNCTION ONLY CONFIGURES THE MODEL\n",
    "# LoRA configuration parameters\n",
    "LORA_R = 8            # Reduced rank for resource constraints\n",
    "LORA_ALPHA = 16       # Scaling factor\n",
    "LORA_DROPOUT = 0.05   # Dropout probability\n",
    "\n",
    "# Define LoRA Config with updated target modules for LLaMA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "# Specify your LLaMA model name\n",
    "#model_name = 'your-llama-model'  # Replace with your actual model name\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ],
   "id": "74c13b4d32b0b80e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 645,120 || all params: 135,160,704 || trainable%: 0.4773\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-25T19:21:08.684958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "# Adjust the model configuration if needed\n",
    "model.config.use_cache = False\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "MODEL_SAVE_FOLDER_NAME = \"C://Users//adity//PyCharmMiscProject//Trained Model 1\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    no_cuda=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "# Training the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "# Save the model and configuration\n",
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n"
   ],
   "id": "8afe8cc401829b45",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_13060\\3812112524.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c45538b996ea28a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
